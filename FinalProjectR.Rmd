---
title: "FinalProjectR"
output: html_document
---

```{r setup, include=FALSE}
setwd("F:/MSCSNyuPoly/Fall2016/FoundationsOfDataScience/Project/Work")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE,warning = F)

```
install.packages("chron")
install.packages("sqldf")
install.packages("rgdal")
install.packages("stringr")
install.packages("rgeos")
install.packages("dplyr")
install.packages("Hmisc")
install.packages("lubridate")
install.packages("caret")
install.packages("knncat")
install.packages("tau")
install.packages("tidytext")
install.packages("ngram")
install.packages("RWeka")


s3 link to download data is https://s3-us-west-2.amazonaws.com/fds2016sahil/data.txt
It is recommended to download data locally first.
```{r read}
library(chron)
library(lubridate)
library(caret)

#df <- read.csv("https://s3-us-west-2.amazonaws.com/fds2016sahil/data.txt")

df <- read.table("data.txt")
```

```{r descriptive statistics of invalid zip code}
#Check number of invalid zip codes present
zipCodeDf <- data.frame(as.character(df$Violation.Location..Zip.Code.))
colnames(zipCodeDf) <- "zipCode"

zipCodeLength <- do.call(data.frame, aggregate(cbind(count = zipCode) ~ nchar(as.character(zipCodeDf$zipCode)), data = zipCodeDf, FUN = function(x) c({NROW(x)})))
colnames(zipCodeLength) <- c("ZipCodeLength", "count")
zipCodeLength
```

```{r}
#only select the rows where decision hearing result is either against the respondent("IN VIOLATION") 
#or for the respondent("DISMISSED")
selected <- c("DISMISSED","IN VIOLATION")
df1 <- df[df$Hearing.Result %in% selected,]
df_unique <- unique(df$Issuing.Agency)
indx <- sapply(df1, is.factor) 
#remove dollar sign
df1[indx] <- lapply(df1[indx], function(x) 
                            as.character(gsub("\\$", "", x)))
#convert every column to factor
df1<-data.frame(lapply(df1,factor))
#convert date time to Datetime Format
df1$Violation.Date<- as.Date(df1$Violation.Date, "%m/%d/%Y")
df1$Violation.Time <- chron(times. = df1$Violation.Time)


newdf <- data.frame(df1[,c("Violation.Date","Violation.Time","Issuing.Agency","Violation.Location..Borough.", "Hearing.Result", "Hearing.Date", "Total.Violation.Amount", "Scheduled.Hearing.Location", "Hearing.Time", "Compliance.Status" )])
#convert hearing date and time to date time format
newdf$Hearing.Time <- chron(times. = newdf$Hearing.Time)
newdf$Hearing.Date <- as.Date(newdf$Hearing.Date,"%m/%d/%Y")
#remove Issuing Agency, Hearing Location and Violation Locations whose violations are less than 500,10,150
newdf <- newdf[!(as.numeric(newdf$Issuing.Agency) %in% which(table(newdf$Issuing.Agency)<500)),]
newdf <- newdf[!(as.numeric(newdf$Scheduled.Hearing.Location) %in% which(table(newdf$Scheduled.Hearing.Location)<10)),]
newdf <- newdf[!(as.numeric(newdf$Violation.Location..Borough.) %in% which(table(newdf$Violation.Location..Borough.)<150)),]
newdf <- droplevels(newdf)
#convert Violation Amount to numeric 
newdf$Total.Violation.Amount <- as.numeric(as.character(newdf$Total.Violation.Amount))
set.seed(22)
pIndex <- createDataPartition(newdf$Issuing.Agency, p = .1,
                                  list = FALSE,
                                  times = 1)

```
      
In our initial exploration of which predictors to use we find that Violation Time, Issuing Agency, Violation Location, Hearing Location, Violation Time and Total Violation Amount have highest coefficient values.    
```{r}
newdf <- newdf[pIndex,]
model1 <-glm(Hearing.Result~.,data = newdf,family = 'binomial')
summary(model1)
```

We select the predictor columns and put into new dataframe and then replace missing values with modes as knn doesn't remove missing values by default.       
```{r}

sampledf<-newdf[ , -which(names(newdf) %in% c("Respondent.Last.Name","Balance.Due","Violation.Date", "Violation.Time", "Violation.Details","Paid.Amount","Charge..1..Code.Description", "Compliance.Status","Hearing.Date.1","Violation.Location..Zip.Code.","Hearing.Date","Penalty.Imposed"))]
sampledf <- droplevels(sampledf)

Mode <- function (x, na.rm) {
    xtab <- table(x)
    xmode <- names(which(xtab == max(xtab)))
    if (length(xmode) > 1) xmode <- ">1 mode"
    return(xmode)
}
#impute missing values with mode...
for (var in 1:ncol(sampledf)) {
  sampledf[is.na(sampledf[,var]),var] <- Mode(sampledf[,var], na.rm = TRUE)
}


sampledf$Total.Violation.Amount <- as.numeric(sampledf$Total.Violation.Amount)
```
 
#Model      
Next we start with implementing the algorithms by first splitting the data into 10 folds for k-fold cross validation. Iteratively each fold is used as test set and remaining 9 are used as train. We find accuracy and execution time for each fold.  
             
##K-NN Classification 
```{r}
require(class)
require(knncat)
idx <- createFolds(sampledf$Hearing.Result,k = 10)
sapply(idx, length)
accuracy <- vector()
timeknn <- vector()
for (i in 1:10) {
  start.time <- Sys.time()
  #knncat tests for each k value given and selects the model with best k value.   
  model <- knncat(sampledf[ -idx[[i]] , ], sampledf[ idx[[i]], ], k=c(1,3,5,7,9,11,13,15,19),classcol = 3)
  end.time <- Sys.time()
  timeknn[i]<-end.time - start.time
  pred <- predict(model,sampledf[ -idx[[i]] , ],sampledf[ idx[[i]], ],train.classcol = 3,newdata.classcol = 3)
  cm <-table(pred,sampledf[ idx[[i]], ]$Hearing.Result)
  accuracy[i]<-sum(diag(cm))/sum(cm)
}
cm
```

       
                        
##Linear Regression
We carry out similar  steps for regression.   
```{r}
accuracylm <- vector()
timelm <- vector()
for (i in 1:10) {
  start.time <- Sys.time()
  model <-glm(Hearing.Result~.,family = binomial,data =sampledf[ -idx[[i]] , ] )
  end.time <- Sys.time()
  timelm[i]<-end.time - start.time
  pred <- predict(model,sampledf[ idx[[i]], ],type = 'response')
  cm <- table(sampledf[ idx[[i]], ]$Hearing.Result, pred>0.5)

  accuracylm[i]<-sum(diag(cm))/sum(cm)
}
cm
```
                              
##Random Forests

We carry out similar  steps for Random Forests.   

```{r}
library(randomForest)
accuracyrf <- vector()
timerf <- vector()
for (i in 1:10) {
  start.time <- Sys.time()
  rf <-randomForest(Hearing.Result~.,data =sampledf[ -idx[[i]] , ] ,importance = TRUE)
  end.time <- Sys.time()
  timerf[i]<-end.time - start.time
  pred <- predict(rf,sampledf[ idx[[i]], ])
  cm <- table(sampledf[ idx[[i]], ]$Hearing.Result, pred)
  accuracyrf[i]<-sum(diag(cm))/sum(cm)
}
cm
```
###Importance Plots.    
The mean decrease in accuracy a variable causes is determined during the out of bag error calculation phase. The more the accuracy of the random forest decreases due to the exclusion (or permutation) of a single variable, the more important that variable is deemed, and therefore variables with a large mean decrease in accuracy are more important for classification of the data. The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. Each time a particular variable is used to split a node, the Gini coefficient for the child nodes are calculated and compared to that of the original node. The Gini coefficient is a measure of homogeneity from 0 (homogeneous) to 1 (heterogeneous). The changes in Gini are summed for each variable and normalized at the end of the calculation. Variables that result in nodes with higher purity have a higher decrease in Gini coefficient.    

A type 1 variable importance plot shows the mean decrease in accuracy, while a type 2 plot shows the mean decrease in Gini. We see both the plot agree that hearing location is the most important feature and Scheduled Hearing Location and Violation Amount are other important features.    

```{r}
trainIndex <- createDataPartition(sampledf$Hearing.Result, p = .8,
                                  list = FALSE,
                                  times = 1)
traindf <- sampledf[trainIndex,]
rf <- randomForest(Hearing.Result~., data=traindf,  importance = TRUE)
varImpPlot(rf)
importance(rf)

```





##SVM Implementation
There are close to 170000 rows in the dataset. Also some description occur less than 10 times. We remove those descriptions. Then similar to the step we did for Issuing Agency we balance split on Code Description and then use that for our ngram analysis. 
```{r}
df2 <- df1[!(as.numeric(df1$Charge..1..Code.Description) %in% which(table(df1$Charge..1..Code.Description)<10)),]
df2 <- droplevels(df2)
pIndex <- createDataPartition(df2$Charge..1..Code.Description, p = .1,
                                  list = FALSE,
                                  times = 1)
dfa <- df2[pIndex,]
```

Let's create document term matrix for svm implementation
```{r}
library(tm)
library(SnowballC)
library(tau)
library(RWeka)
descriptions <- data.frame((dfa$Charge..1..Code.Description))
descriptions <- as.character(descriptions$X.dfa.Charge..1..Code.Description.)
documents <- VCorpus(VectorSource(descriptions))
documents <- tm_map(documents,
                     content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
                     mc.cores=1)
#remove symbols
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
documents <- tm_map(documents, toSpace, "/",mc.cores = 1)
documents <- tm_map(documents, toSpace, "\\|", mc.cores = 1)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
documents <- tm_map(documents, content_transformer(removeURL))
#convert to lowercase
documents <- tm_map(documents, content_transformer(tolower),mc.cores = 1)
# Remove numbers
documents <- tm_map(documents, removeNumbers,mc.cores = 1)

# Remove punctuations
documents <- tm_map(documents, removePunctuation,mc.cores = 1)

# Remove english common stopwords
documents <- tm_map(documents, removeWords, stopwords("english"))
# specify your stopwords as a character vector
#documents <- tm_map(documents, removeWords, c("pizza","tco","eat")) 
# Eliminate extra white spaces
documents <- tm_map(documents, stripWhitespace, mc.cores = 1)
# Text stemming
documents <- tm_map(documents, stemDocument, lazy = T) 
#create unigram matrix
dtm <-(DocumentTermMatrix(documents))
```

```{r}
#Bigrams
BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min=2, max=2))}
options(mc.cores=1)
dtm2 <- DocumentTermMatrix(documents, control=list(tokenize=BigramTokenizer))
#Trigrams
#ThreegramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min=3, max=3))}
#options(mc.cores=1)
#dtm3 <- DocumentTermMatrix(documents, control=list(tokenize=ThreegramTokenizer))
NgramTokenize <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min=1, max=2))}
dtmn <- DocumentTermMatrix(documents, control=list(tokenize=NgramTokenize))
trainIndex <- createDataPartition(dfa$Hearing.Result, p = .75,
                                  list = FALSE,
                                  times = 1)

```

```{r}
m1 <- as.matrix(dtm)#unigram
m2 <- as.matrix(dtm2)#bigram
mn <- as.matrix(dtmn)
df1 <- data.frame(m1,dfa$Hearing.Result) #unigramdf
df2 <- data.frame(m2,dfa$Hearing.Result) #bigramdf
dfn <- data.frame(mn,dfa$Hearing.Result) #bigramdf

names(df1)[names(df1) == 'dfa.Hearing.Result'] <- 'Result'
names(df2)[names(df2) == 'dfa.Hearing.Result'] <- 'Result'
names(dfn)[names(dfn) == 'dfa.Hearing.Result'] <- 'Result'
traindf1 <- df1[trainIndex,]
testdf1 <- df1[-trainIndex,]
traindf2 <- df2[trainIndex,]
testdf2 <- df2[-trainIndex,]

library("e1071")
library("kernlab")
svm_model1 <- svm(Result ~ ., data=traindf1) #unigramdf
pred1 <- predict(svm_model1,testdf1)
cm1 <- table(pred1,testdf1$Result)
cm1
sum(diag(cm1))/sum(cm1) #Accuracy unigram linear svm
svm_model2 <- svm(Result ~ ., data=traindf2)#bigramdf
pred2 <- predict(svm_model2,testdf2)
cm2 <- table(pred2,testdf2$Result)
cm2
sum(diag(cm2))/sum(cm2) #Accuracy bigram linear svm
```

The  accuracy for unigram is clearly low. So let's change the kernel from linear to rbf 
```{r}
#rbf kernel
svp <- ksvm(Result~., data= traindf1,type="C-svc",kernel='rbf',kpar=list(sigma=1),C=1)
pred1 <- predict(svp,testdf1)
cm1 <- table(pred1,testdf1$Result)
cm1
sum(diag(cm1))/sum(cm1)#Accuracy unigram rbf kernel
svp <- ksvm(Result~., data= traindf2,type="C-svc",kernel='rbf',kpar=list(sigma=1),C=1)
pred2 <- predict(svp,testdf2)
cm2 <- table(pred2,testdf2$Result)
cm2
sum(diag(cm2))/sum(cm2) #Accuracy Bigram rbf
```
      
We observe that the accuracy for unigram and bigram improves by about 0.05 when we change the kernel.



#Visualization
First Visualitation is for the Accuracies of 3 of the algorithms i.e Logistic Regression, Random Forest, K-NN Classification.It is a grouped bar plot and we observe the accuracy of Random Forest is slightly better than the other two.    
##Accuracy Comparison    
```{r}
library(reshape2)
folds <- 1:10
mean(accuracyrf)#random forest accuracy averaged across folds
mean(accuracylm)#logistic regression acccuracy averaged across folds
mean(accuracy)#knn accuracy averaged across folds...
comp<-data.frame(accuracy,accuracylm,accuracyrf,folds)
colnames(comp)<- c("knn","logistic regresion","Random Forests", "folds")
compfull <- melt(comp,id = c("folds"))
colnames(compfull) <- c("folds","Model","Accuracy")
compfull$folds <- as.factor(compfull$folds)
ggplot(compfull, aes(factor(folds), Accuracy, fill = factor(Model)))+
  labs(title="Accuracy Comparison") +
  coord_cartesian(ylim=c(0.7,0.8)) +
  theme(plot.title = element_text(hjust = 0.5))+ 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set1")
```
                              
##Execution Time Comparison    
Secondly we identify the execution time of each algorithm and plot them on the grouped bar plot. We find the logistic regression performs the best followed by random forest and knn.         
```{r}
folds <- 1:10
comp<-data.frame(timeknn,timelm,timerf,folds)
colnames(comp)<- c("knn","logistic regresion","Random Forests", "folds")
compfull <- melt(comp,id = c("folds"))
colnames(compfull) <- c("folds","Model","Time")
compfull$folds <- as.factor(compfull$folds)
ggplot(compfull, aes(folds, Time, fill = Model))+labs(title="Running Time Comparison") +ylim(0,35)+
  theme(plot.title = element_text(hjust = 0.5))+ 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set1")
```
                 
                 
##Issuing Agencies Comparison    
We plot stacked bar plot of top Issuing Agencies with The violation count on the y axis and the hearing result as the differentiatior. This graph helps us identify the issuing agencies who are more consistent in their Violation tickets and the we can also find out which agency's violations are dismissed more frequently which signifies they have higher error rate. We observed here that Sanitation issuing agencie's violations were dismissed more than 50% of time.
```{r}
dfbar <- sampledf[!(as.numeric(sampledf$Issuing.Agency) %in% which(table(sampledf$Issuing.Agency)<500)),]
dfbar <- droplevels(dfbar)
issuingagency<-data.frame(unique(dfbar$Issuing.Agency),unique(abbreviate(dfbar$Issuing.Agency)))
colnames(issuingagency)<-c("Name","Abbreviation")
issuingagency
qplot(abbreviate(Issuing.Agency), data=dfbar,geom="bar", fill=(Hearing.Result))+labs(title = "Violations issued by Issuing Agency")

```

##Contribution of n-grams to the outcome
The y axis shows the relevant terms that are important in determining the Hearing Results...
```{r}
library(dplyr)
library(tidytext)

classes <- data.frame(rownames(dfa),dfa$Hearing.Result)
colnames(classes) <- c("document","result")
dtmn1 <- tidy(dtmn)
mergedtmn <- merge(dtmn1,classes)
dismissed<- dfn[dfn$Result %in% c("DISMISSED"),]
violation <- dfn[dfn$Result %in% c("IN VIOLATION"),]
dtmndis <-as.matrix( dtmn[rownames(dismissed),])
dtmnvio <- as.matrix(dtmn[rownames(violation),])
freqd <- sort(colSums(as.matrix(dtmndis)), decreasing=TRUE)   
freqv <- sort(colSums(as.matrix(dtmnvio)), decreasing=TRUE)   
head(freqd, 10) #10 most frequent features for Dismissed
head(freqv, 10) #10 most frequent features for Violation
mergedtmn %>%
  count(result, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 40) %>%
  mutate(n = ifelse(result == "IN VIOLATION", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = result)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to Hearing Result")
```

#Find Optimum Penalty Amount

```{r select data for optimum amount}
data2015 <- df
#Total records in dataset
nrow(data2015)
data2015$Hearing.Status <- as.character(data2015$Hearing.Status)
data2015$Hearing.Result <- as.character(data2015$Hearing.Result)
data2015$Decision.Date <- as.Date(data2015$Decision.Date, format = "%m/%d/%Y")
data2015 <- subset(data2015, Total.Violation.Amount != 0)

```

Finding percentage of violations for each amount
```{r optimum amount complete data}
#Select only Total Violation Amount column
data2015Amt <- subset(data2015, Total.Violation.Amount != 0, select = c("Total.Violation.Amount"))
#Total number of violations
ttlViolation <- nrow(data2015Amt)
#Aggregation to find percentage occurenece of each penalty amount
options(scipen = 999)  #To disable scietific notation
ttlAgg <- do.call(data.frame, aggregate(cbind(count = Total.Violation.Amount) ~ Total.Violation.Amount, data = data2015Amt, FUN = function(x) c({round((NROW(x) / ttlViolation) * 100, digits = 4)}, {NROW(x)})))
colnames(ttlAgg) <- c("Total.Violation.Amount", "ttlPercent", "ttlCnt")
```


Find percentage of violations for which amount was paid before decision for each amount
```{r optimum amount paid data}
#Select data where Hearing Status is PAID IN FULL and Decision Date is blank. It means that amount has been paid in full before taking any decision
data2015Paid <- subset(data2015, Hearing.Status == "PAID IN FULL" & is.na(Decision.Date) & Total.Violation.Amount != 0, select = c("Total.Violation.Amount"))
#Aggregation  to find percentage occurence of penalty where user has paid fully
paidAgg <- do.call(data.frame, aggregate(cbind(count = Total.Violation.Amount) ~ Total.Violation.Amount, data = data2015Paid, FUN = function(x)c({round((NROW(x) / ttlViolation) * 100, digits = 4)}, {NROW(x)})))
colnames(paidAgg) <- c("Total.Violation.Amount", "paidPercentFromTotal", "paidCount")
```

Join both datasets
```{r join optimum amount}
#Left join of ttl Percentage and paid percentage
mergeData <- merge(x = ttlAgg, y = paidAgg, by = "Total.Violation.Amount", all.x = TRUE)
#Replace NA with 0
mergeData <- data.frame(apply(mergeData, 2, function(x){replace(x, is.na(x), 0)}))
#Calculate percentage violations per amount where penalty was paid before decision
mergeData$paidPercentPerPenalty <- (mergeData[,"paidCount"] / mergeData[,"ttlCnt"]) * 100
head(mergeData)

```

Plot data
Barplot of amount versus total number of violations for that amount
```{r plot optimum amount}
#Barplot
rownames(mergeData) <- mergeData$Total.Violation.Amount
barplot(t(as.matrix(mergeData[,c(3, 5)])), col=c("darkblue","red"), legend=c("Total", "Paid"),ylim=c(0, 320000), xlab = "Amount", main = "Amount vs Count of Total & Amount Paid before Decision", beside=TRUE)
```


We are not able to see proper result because of large data. Even with barplot we are not able to visualize properly. We can divide data into bins to get overall estimation.

```{r divide into bins}
library(Hmisc)
mergeData$amountGrp <- cut2(mergeData$Total.Violation.Amount, c(101, 1001, 2501, 5001, 10001, 100000 ))
mergeDataAgg <- aggregate(mergeData[,c("ttlCnt","paidCount")], by=list(mergeData$amountGrp), "sum")
colnames(mergeDataAgg) <- c("Violation.Amount.Grp",  "ttlCnt", "paidCount")

barPlotData <- t(as.matrix(mergeDataAgg[,c(2,3)]))
names(barPlotData) <- mergeDataAgg[,c(1)]
bar <- barplot(barPlotData, col=c("darkblue","red"), legend=c("Total", "Paid"),ylim=c(0, 420642),ylab = "Count", xlab = "", main = "Amount vs Count of Total & Amount Paid before Decision", beside=TRUE)
axis(1, at=bar[1,], labels=mergeDataAgg[,c(1)], font = 2, cex.axis=0.7, las=3)
```

Plot of amount grouped in bins versus total violations for that bin.
We can see from plot that maximum number of times amount was paid before decision when penalty amount was less than or equal to 100.

So lets zoom in there.
```{r zoom in}
barplot(t(as.matrix(mergeData[mergeData$Total.Violation.Amount <= 100,c(3, 5)])), col=c("darkblue","red"),ylim=c(0, 320000), xlab = "Amount", main = "Amount vs Count of Total & Amount Paid before Decision", beside=TRUE)
```
Plot of amount less than 100 versus total violations for each amount


We can see that for lower penalty amount there is more percentage to pay penalty beofre decison. It is highest for penalty amount $25.


#Revenue Lost over Defaulted Penalties
```{r Revenue Lost}
#How much revenue is lost on defaulted penalties
copyData2015 <- data2015
#Extract month from violation date
copyData2015$Violation.Date <- format(as.Date(as.character(copyData2015$Violation.Date), "%m/%d/%Y"), "%m")
#Select defaulted violations
data15MonthDefaulted <- subset(copyData2015, (Hearing.Status == "DOCKETED" | Hearing.Status == "DEFAULTED") & Hearing.Result == "DEFAULTED", select = c("Violation.Date", "Total.Violation.Amount"))

#Select subset of data for all violations
data15Month <- subset(copyData2015, select = c("Violation.Date", "Total.Violation.Amount"))

#Aggregate default violations
dataMonthDeafaultedAgg <- do.call(data.frame, aggregate(cbind(sum = Total.Violation.Amount) ~ Violation.Date, data = data15MonthDefaulted, FUN = sum))
colnames(dataMonthDeafaultedAgg) <- c("Violation.Month", "DefaultedSum")
#Aggregate all violations
data15MonthAgg <- do.call(data.frame, aggregate(cbind(sum = Total.Violation.Amount) ~ Violation.Date, data = data15Month, FUN = sum))
colnames(data15MonthAgg) <- c("Violation.Month", "ttlSum")
#Join and calculate percentage
mergeData <- merge(x = data15MonthAgg, y = dataMonthDeafaultedAgg, by = "Violation.Month", all.x = TRUE)
mergeData$Violation.Month <- as.character(mergeData$Violation.Month)
(mergeData)

#Plot bargraph
rownames(mergeData) <- mergeData$Violation.Month
barplot(t(as.matrix(mergeData[,c(2,3)])), xlab = "Month", col=c("darkblue","red"), main = "Monthwise Total Revenue and lost revenue", beside=TRUE)

legend("topleft", legend=c("Total", "Defaulted"), pch=20, col=c("darkblue","red"), horiz=TRUE, bty='n', cex=0.8)
```

Above plot shows month versus total revenue and lost revenue over defaulted penalties.




# Find Percentage of Defaulted Violations for Different Features
```{r aggregate on diff features}
copyData2015 <- data2015
#Select defaulted violations
data15Defaulted <- subset(copyData2015, (Hearing.Status == "DOCKETED" | Hearing.Status == "DEFAULTED") & Hearing.Result == "DEFAULTED", select = c("Charge..1..Code.Description", "Violation.Location..Zip.Code.", "Respondent.Address..Zip.Code.", "Total.Violation.Amount"))
#Select subset of data for all violations
data15All <- subset(copyData2015, select = c("Charge..1..Code.Description", "Violation.Location..Zip.Code.", "Respondent.Address..Zip.Code.", "Total.Violation.Amount"))

#Aggregate default violations
dataDeafaultedAgg <- do.call(data.frame, aggregate(cbind(DefaultedCount = Total.Violation.Amount) ~ (Charge..1..Code.Description + Violation.Location..Zip.Code. + Respondent.Address..Zip.Code. + Total.Violation.Amount), data = data15Defaulted, FUN = function(x)c({NROW(x)})))

#Aggregate all violations
data15AllAgg <- do.call(data.frame, aggregate(cbind(TotalCount = Total.Violation.Amount) ~ (Charge..1..Code.Description + Violation.Location..Zip.Code. + Respondent.Address..Zip.Code. + Total.Violation.Amount), data = data15All, FUN = function(x)c({NROW(x)})))


#Group by all columns
mergeData <- merge(x = data15AllAgg, y = dataDeafaultedAgg, by = c("Charge..1..Code.Description", "Violation.Location..Zip.Code.", "Respondent.Address..Zip.Code.", "Total.Violation.Amount"), all.x = TRUE)
mergeData <- data.frame(apply(mergeData, 2, function(x){replace(x, is.na(x), 0)}))
mergeData$TotalCount <- as.numeric(as.character(mergeData$TotalCount))
mergeData$DefaultedCount <- as.numeric(as.character(mergeData$DefaultedCount))
mergeData$Violation.Location..Zip.Code. <- (as.character(mergeData$Violation.Location..Zip.Code.))
mergeData$Respondent.Address..Zip.Code. <- (as.character(mergeData$Respondent.Address..Zip.Code.))
```

```{r agg by violation zip code}
#Agg by Violation zip code
mergeDataAgg <- aggregate(mergeData[,c("TotalCount", "DefaultedCount")], by = list(mergeData$Violation.Location..Zip.Code.), "sum")
colnames(mergeDataAgg) <-c("Violation.Location..Zip.Code.", "TotalCount", "DefaultedCount")
mergeDataAgg <- mergeDataAgg[(nchar(mergeDataAgg$Violation.Location..Zip.Code.) == 5 & mergeDataAgg$TotalCount >= 100),]
mergeDataAgg$Percentage <- (mergeDataAgg$DefaultedCount/ mergeDataAgg$TotalCount) * 100
top10 <- head(mergeDataAgg[order(mergeDataAgg$Percentage, decreasing = T), c(1,4)], 10)
top10
```

```{r aggregate by respondent zip code}
#Agg by respondent Zip code
mergeDataAgg <- aggregate(mergeData[,c("TotalCount", "DefaultedCount")], by = list(mergeData$Respondent.Address..Zip.Code.), "sum")
colnames(mergeDataAgg) <-c("Respondent.Address..Zip.Code.", "TotalCount", "DefaultedCount")
mergeDataAgg <- mergeDataAgg[(nchar(mergeDataAgg$Respondent.Address..Zip.Code.) == 5 & mergeDataAgg$TotalCount >= 100),]
mergeDataAgg$Percentage <- (mergeDataAgg$DefaultedCount/ mergeDataAgg$TotalCount) * 100
top10 <- head(mergeDataAgg[order(mergeDataAgg$Percentage, decreasing = T), c(1,4)], 10)
top10
```



```{r agg by amount}
#Agg by Amount
mergeDataAgg <- aggregate(mergeData[,c("TotalCount", "DefaultedCount")], by = list(mergeData$Total.Violation.Amount), "sum")
colnames(mergeDataAgg) <-c("Total.Violation.Amount", "TotalCount", "DefaultedCount")
mergeDataAgg <- mergeDataAgg[(mergeDataAgg$TotalCount >= 100),]
mergeDataAgg$Percentage <- (mergeDataAgg$DefaultedCount/ mergeDataAgg$TotalCount) * 100
top10 <- head(mergeDataAgg[order(mergeDataAgg$Percentage, decreasing = T), c(1,2, 4)], 10)
top10
```


# Clean Neighborhoods

Find cleanliness related words and filter data based on those words

```{r read clean violation data}
#Find out all violations related data from 
dataCleanLabel <- subset(data2015, Dirty.Label == 1 & Charge..1..Code.Description != "", select = c("Charge..1..Code.Description"))
library(tm)
dataCleanLabel <- as.vector(dataCleanLabel)
dataCleanCorpus <- VCorpus(VectorSource(dataCleanLabel))
dataCleanCorpus <- tm_map(dataCleanCorpus, removeWords, stopwords("english"))
dataCleanCorpus <- tm_map(dataCleanCorpus,  removePunctuation)
dataCleanCorpus <- tm_map(dataCleanCorpus, content_transformer(tolower))
dataCleanCorpus <- tm_map(dataCleanCorpus, stripWhitespace)

library(ngram)
library(RWeka)
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
dtm <- DocumentTermMatrix(dataCleanCorpus, control = list(tokenize = UnigramTokenizer, wordLengths = c(3,10)))

dtmFrame <- as.data.frame( t(as.matrix(  dtm )) ) 
dtmFrame$Term <- rownames(dtmFrame)
colnames(dtmFrame) <- c("count", "Term")

top30 <- head(dtmFrame[order(dtmFrame$count, decreasing = T), c(1, 2)], 30)
top30
#We uused unigram to verify words selected for selecting all the violations related to cleanliness

dataClean <- subset(data2015, Hearing.Result == "IN VIOLATION")
words <- c("clean", "receptacle", "dirty", "rubbish", "dirt", "waste", "garbage", "litter", "disposal", "dumping", "mosquito", "smoke")
dataClean <- dataClean[ifelse (rowSums(sapply(words, grepl, data2015$Charge..1..Code.Description, ignore.case = TRUE)) > 0, TRUE, FALSE),]
library(stringr)
dataClean$Charge..1..Code.Description <- str_trim(dataClean$Charge..1..Code.Description)

#Group by violation zip code and take count
dataCleanAgg <- do.call(data.frame, aggregate(cbind(count = Violation.Location..Zip.Code.) ~ Violation.Location..Zip.Code., data = dataClean, FUN = function(x) c({NROW(x)})))
colnames(dataCleanAgg) <- c("ZIPCODE", "count")
library(ggplot2)
library(Hmisc)
dataCleanAgg <- subset(dataCleanAgg, nchar(as.character(dataCleanAgg$ZIPCODE)) == 5)
dataCleanAgg$countGrp <- cut2(dataCleanAgg$count, c(0, 1, 100, 300, 600, 1000, 4000))
```


read shape file data
You can either use attached ZIP_CODE_040114 folder or download data from https://github.com/mugdhachaudhari/ExternalFactorEffectOnOATHHearing/tree/master/ZIP_CODE_040114

```{r read shape file data}
library(rgdal)
nyc <- readOGR(dsn = "ZIP_CODE_040114", layer = "ZIP_CODE_040114")
nyc$id <- row.names(nyc)

#Left join with shape file data Ignoring Blank, 0 zipcodes from violation data
nyc@data <- merge(x = nyc@data, y = dataCleanAgg, by = "ZIPCODE", all.x = TRUE)
#Replacing NAs with group 1 to include 0 violations.
nyc@data <- data.frame(apply(nyc@data, 2, function(x){replace(x, is.na(x), "   0")}))

#Plot map
library(rgeos)
nyc_f <- fortify(nyc)
library(dplyr)
nyc_f <- left_join(nyc_f, nyc@data)
map <- ggplot() +
geom_polygon(data = nyc_f, aes(x = long, y = lat, group = group, fill = countGrp), color = "black", size = 0.25) +
coord_equal() +
labs(fill = "Number of Cleanliness\nRelated Violations 2015") +
ggtitle("Clean Neighborhoods")
map
```

We can see in this map that neighborhoods with high cleanliness related violations(Sky blue, Blue) are clustered together.  Plotting this data on map helped us to identify those clusters and based on it we can narrow down the factors contributing to it.
Manhattan seems to be most unsanitary neighborhood which is correlated with high population of manhattan.



Generate all dates for year 2015
```{r all dates}
#Generate all dates for year 2015
dateSeq <- data.frame(seq(as.Date("2015/01/01"), as.Date("2015/12/31"), by = "day"))
colnames(dateSeq) <- c("Violation.Date")
dateSeq$Violation.Date <- as.Date(dateSeq$Violation.Date, format = "%Y-%m-%d")
```

Pick top two zipcodes where violations are most.
```{r pick top two zipcodes}
head(dataCleanAgg[order(as.numeric(dataCleanAgg$count), decreasing = T), ], 2)
```

Filter data for zipcode 10019
```{r timeseries for zipcode 10019}
dataCleanSub <- subset(dataClean, Violation.Location..Zip.Code. == 10019, select = c("Violation.Date"))
dataCleanSub$Violation.Date <- as.Date(as.character(dataCleanSub$Violation.Date), "%m/%d/%Y")
dataCleanSubCnt <- data.frame(table(dataCleanSub))
colnames(dataCleanSubCnt) <- c("Violation.Date", "count")
dataCleanSubCnt$Violation.Date <- as.Date(dataCleanSubCnt$Violation.Date, format = "%Y-%m-%d")
mergeDateViolations <- merge(x = dateSeq, y = dataCleanSubCnt, by = "Violation.Date", all.x = TRUE)
#Replace NA with 0
mergeDateViolations <- data.frame(apply(mergeDateViolations, 2, function(x){replace(x, is.na(x), 0)}))
```

Plot timeseries for 10019
```{r plot ts for 10019}
#Daily
plot(ts(mergeDateViolations$count, start = c(2015, 1), frequency = 365), ylab = "count", main = "Zip Code 10019 Daily timeseries")
#Monthly
mergeDateViolations$Month <- 
format(as.Date(mergeDateViolations$Violation.Date), "%m-%Y")
mergeDateViolations$count <- as.numeric(as.character(mergeDateViolations$count))

mergeMonthViolations <- aggregate(mergeDateViolations[,c("count")], by=list(mergeDateViolations$Month), "sum")
colnames(mergeMonthViolations) <- c("Month", "count")

plot(ts(mergeMonthViolations$count, start = c(2015, 1), frequency = 12), ylab = "count", main = "Zip Code 10019 Monthly timeseries")
```

Above two timeseries are daily and monthly time series for zip code 10019. We can conclude from monthly time series that cleanlinees related violations are low during start of year and it is increasing in periods during rest of the year.



Decompose time series for zip code 10019
```{r decomposition for zip code 10019}
library(forecast)
weeklyAndMonthly <- msts(mergeDateViolations$count, seasonal.periods = c(7, 30, 365.25))
seasonalPattern <- tbats(weeklyAndMonthly)
#Zipcode 10019
plot(seasonalPattern)
```

It shows decomposition of timeseries. tbats helps us to capture dynamic multi seasonality.  Season 1, season 2, season 3 indicates weekly, monthly and yearly seasonality respectively. We can conclude from weekly timeseries that there is peak at each week. It might be because issuing agency would be reporting violations on any particular day of a week. 


Filter data for zip code 10003
```{r timeseries for zipcode 10003}
dataCleanSub <- subset(dataClean, Violation.Location..Zip.Code. == 10003, select = c("Violation.Date"))
dataCleanSub$Violation.Date <- as.Date(as.character(dataCleanSub$Violation.Date), "%m/%d/%Y")
dataCleanSubCnt <- data.frame(table(dataCleanSub))
colnames(dataCleanSubCnt) <- c("Violation.Date", "count")
dataCleanSubCnt$Violation.Date <- as.Date(dataCleanSubCnt$Violation.Date, format = "%Y-%m-%d")
mergeDateViolations <- merge(x = dateSeq, y = dataCleanSubCnt, by = "Violation.Date", all.x = TRUE)
#Replace NA with 0
mergeDateViolations <- data.frame(apply(mergeDateViolations, 2, function(x){replace(x, is.na(x), 0)}))
```


Plot timeseries for zip code 10003
```{r plot ts for 10003}
#Daily
plot(ts(mergeDateViolations$count, start = c(2015, 1), frequency = 365), ylab = "count", main = "Zip Code 10019 Daily timeseries")
#Monthly
mergeDateViolations$Month <- 
format(as.Date(mergeDateViolations$Violation.Date), "%m-%Y")
mergeDateViolations$count <- as.numeric(as.character(mergeDateViolations$count))

mergeMonthViolations <- aggregate(mergeDateViolations[,c("count")], by=list(mergeDateViolations$Month), "sum")
colnames(mergeMonthViolations) <- c("Month", "count")

plot(ts(mergeMonthViolations$count, start = c(2015, 1), frequency = 12), ylab = "count", main = "Zip Code 10019 Monthly timeseries")
```

Above two timeseries are daily and monthly time series for zip code 10003. We can conclude from monthly time series that cleanlinees related violations are low during start of year and it is increasing in periods during rest of the year and there is again low point in the middle of summer.


Decompose time series for zip code 10003
```{r decomposition for zip code 10003}
library(forecast)
weeklyAndMonthly <- msts(mergeDateViolations$count, seasonal.periods = c(7, 30, 365.25))
seasonalPattern <- tbats(weeklyAndMonthly)
#Zipcode 10019
plot(seasonalPattern)
```

It is same as zip code 10019 which we have show before.
It shows decomposition of timeseries. tbats helps us to capture dynamic multi seasonality.  Season 1, season 2, season 3 indicates weekly, monthly and yearly seasonality respectively. We can conclude from weekly timeseries that there is peak at each week. It might be because issuing agency would be reporting violations on any particular day of a week. 